{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intro loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load torchvision & ivy models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 20:31:20.671716: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-21 20:31:21.196860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING:root:To preserve the compiler and transpiler caches across multiple machines, ensure that the relative path of your projects from the .ivy folder is consistent across all machines. You can do this by adding .ivy to your home folder and placing all projects in the same place relative to the home folder on all machines.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import ivy\n",
    "import json\n",
    "import os\n",
    "from utils import download_weights\n",
    "from efficientnetv1 import EfficientNetV1\n",
    "\n",
    "ivy.set_torch_backend()\n",
    "\n",
    "# Load torchvision model\n",
    "torch_model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "torch_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load Ivy model\n",
    "device = \"cpu\"\n",
    "with open(\"variant_configs.json\") as json_file:\n",
    "    configs = json.load(json_file)\n",
    "configs = configs[\"v1\"]\n",
    "base_model = configs[\"base_args\"]\n",
    "phi_values = configs[\"phi_values\"][\"b0\"]\n",
    "def create_model():\n",
    "    ivy_model = EfficientNetV1(\n",
    "        base_model, phi_values, 1000, device=device, training=False\n",
    "    )\n",
    "\n",
    "    # copy weights\n",
    "    weight_path = \"weights/b0.pickled\"\n",
    "    if not os.path.isfile(weight_path):\n",
    "        download_weights(weight_path)\n",
    "\n",
    "    torch_v = ivy.Container.cont_from_disk_as_pickled(weight_path)\n",
    "    torch_list_weights = torch_v.cont_to_flat_list()\n",
    "    assert len(ivy_model.v.cont_to_flat_list()) == len(torch_list_weights)\n",
    "\n",
    "    ivy_model.v.classifier.submodules.v1.b = ivy.Array(\n",
    "        torch_list_weights[0].detach().cpu().numpy()\n",
    "    ).to_device(device)\n",
    "    ivy_model.v.classifier.submodules.v1.w = ivy.Array(\n",
    "        torch_list_weights[1].detach().cpu().numpy()\n",
    "    ).to_device(device)\n",
    "    del torch_list_weights[:2]\n",
    "\n",
    "    def _copy_weights(dictionary):\n",
    "        for key, value in list(dictionary.items()):\n",
    "            # print(key, type(value))\n",
    "            if isinstance(value, dict):\n",
    "                _copy_weights(value)\n",
    "            else:\n",
    "                assert (\n",
    "                    torch_list_weights[0].shape == value.shape\n",
    "                ), f\"{torch_list_weights[0].shape}, {value.shape}\"\n",
    "                dictionary.pop(key)\n",
    "                dictionary[key] = ivy.Array(\n",
    "                    torch_list_weights[0].detach().cpu().numpy()\n",
    "                ).to_device(device)\n",
    "                del torch_list_weights[0]\n",
    "    for k, v in ivy_model.v.features.submodules.items():\n",
    "        _copy_weights(v)\n",
    "    assert len(torch_list_weights) == 0\n",
    "    return ivy_model\n",
    "\n",
    "ivy_model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"images/ILSVRC2012_test_00000007.jpeg\"\n",
    "image_path = 'images/ILSVRC2012_test_00000030.jpeg'\n",
    "# Define the transformation pipeline\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            256, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "        ),  # Resize the image to a square of size 256x256\n",
    "        transforms.CenterCrop(224),  # Crop the center portion of the image to 224x224\n",
    "        transforms.ToTensor(),  # Convert the PIL image to a tensor\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # Normalize the image\n",
    "    ]\n",
    ")\n",
    "\n",
    "image = Image.open(image_path)\n",
    "input_tensor = preprocess(image).unsqueeze(0)  # Add a batch dimension\n",
    "numpy_image = input_tensor.detach().cpu().numpy()\n",
    "numpy_image = numpy_image.reshape((1, 224, 224, 3))\n",
    "\n",
    "ivy_image = ivy.Array(numpy_image).to_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ivy.array(902),\n",
       " [ivy.array(0.02553806),\n",
       "  ivy.array(0.03418851),\n",
       "  ivy.array(0.06302106),\n",
       "  ivy.array(0.08793806),\n",
       "  ivy.array(0.13497415)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = ivy.softmax(ivy_model(ivy_image))\n",
    "output[0].argmax(), sorted(output[0])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(97),\n",
       " [tensor(0.0007),\n",
       "  tensor(0.0007),\n",
       "  tensor(0.0024),\n",
       "  tensor(0.0029),\n",
       "  tensor(0.9771)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = F.softmax(torch_model(input_tensor), dim=1)\n",
    "output[0].argmax(), sorted(output[0])[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ablation study?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ivy.array([-0.14932327, -0.89016241,  1.44651437, -0.38861609, -1.47516894])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivy_model.features._submodules[0].conv._submodules[0](ivy_image).reshape(shape=(1, 32, 112, 112))[0][0][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if equal proves both models are receiving the same input\n",
    "torch_model.features[0][0](input_tensor)[0][0][0][:5] == torch_model.features[0][0](torch.Tensor(numpy_image).reshape(1, 3, 224, 224))[0][0][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.9799, -6.0744, -5.7570, -5.8083, -5.2820], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_model.features[0][0](input_tensor)[0][0][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('ivy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c714e95605b85a09205ce21aadf98d215eb42d39abdf48609eed49589b4df3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
